# Module 1: Let’s ingest some files into the index (no coding required) 

This module requires no coding but it will allow you to ingest a set of files (clinical-trials), extract both structured and unstructured text from those files, and index their content.
Using the Portal Import Data Flow:
0.	Navigate to your search service, and click the Import Data button. This will launch the Import Data Wizard.

 

1.	Create data source called: clinical-trials-mini  that has 10 or so sample files.
 
Pick Azure Blob Storage, and navigate to your clinical-trials-small container.
Name it clinical-trials-small, and then click on “Choose existing connection”
 

Select the clinical-trials-small container.
 

Now click Next to add cognitive enrichments to it.
2.	Skillset
a.	Attach the Cognitive Service resource you created earlier to this enrichment process. It will be used to power any of your pre-built AI models.
 
b.	Add enrichments:
Name your skillset: clinical-trials-small 

c.	Make sure to select the OCR enrichment to extract merged_content field, then let’s actually apply an enrichment to the merged_content field to extract the locations.  For consistency’s sake, let’s call the field name locations. (Lowercase l)



d.	Add a Knowledge store. Click choose an existing connection, select your storage account and create a container called clinical-trials-small-ks


 

Click Next to customize the target index.

3.	In the index definition, rename to it to clinical-trials-small,
Make sure all the field are retrievable. 
Make sure that the locations field is retrievable / facetable / filterable / searchable
Optional: You can make layoutText not searchable/retrievable since we won’t need it for this workshop.

 

4.	Also name the indexer clinical-trials-small.
We only need to run it once for now. Note that the index key is encoded by default. (Leave it that way)
 

Wait 2 or 3 minutes or so for the indexing to occur – then go check the status of your indexer on the portal.  
 


 
After having indexed clinical trials using the portal experience, let’s go query the service.
 

1.	By the way, let’s visualize what the knowledge store looks like.
Let’s see the tables:
We see two tables, one for the documents and another one for each of the entities identified in those documents… 
 

 

This data could be useful for analytics, to train an ML model, or simply to maintain a cache of any extractions we produced. In Module 3 we will create a visualization using data from the Knowledge Store.
